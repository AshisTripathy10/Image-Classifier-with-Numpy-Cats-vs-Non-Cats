Introduction: In this project, we build a cat vs non-cat classifier using logistic regression, while only using numpy. No other package like tensorflow or keras is used here. We are given a set of images of cats and other animals.

Importing Libraries: We import Numpy, Matplotlib‚Äôs Pyplot and h5py libraries. The h5py library in Python provides an interface to the HDF5 binary data format, enabling efficient storage and manipulation of large, complex datasets.

Loading the Data: The dataset is in .h5 file. It is a file format that could store the data - along with its meta-data - in the form of a hierarchy. We access it using the h5py.File function and finally store the data in the form of numpy arrays.

Split the Data: The test data contains 50 samples. We split the first 25 samples to form the validation data, while the rest 25 samples form the test data. We check the shape of our datasets and use the np.squeeze() to specifically view the number of elements in each

Exploring the Data: The classes of the images in the datasets are cat and non-cat, where label 1 means 'cat' and 0 means 'non-cat'. We print the classes, and how the labels of the classes are stored in the data set. Then, we plot an image using pyplot‚Äôs imshow()

Shape of the Data: We check the shape of the training, validation and testing data. The feature sets return the number of images in the datasets along with the shape of each image. Each image is of shape (64, 64, 3) where 3 is for the 3 channels (RGB), and 64 is the height and width of the image. The label sets return the number of labels, which is equal to the number of images

Reshaping the Data: We reshape the data with reshape(train_set_x_orig.shape[0], -1), so that each image is flattened into a vector. Transposing this matrix using .T aligns the data so that each column represents a flattened image, and each row corresponds to a pixel value across all images, making it compatible with machine learning algorithms like Logistic Regression. After transposing, train_set_x_flatten has a shape of (12,288, 209), meaning there are 12,288 (=64*64*3) features (pixels) for 209 training examples.

Feature Scaling: The values of the pixels in RGB images range from 0 to 255. We apply Scaling to these features values for our ML algorithms to work fine on them. We divide the values by 255 to bring them in the range of [0,1]

Defining Utility Functions: We define some helper functions to be used in our algorithm. These functions together implement the Logistic Regression classifier. The workflow involves initializing the model parameters, performing forward and backward propagation to compute predictions and gradients, updating the model through optimization, and finally making predictions on new data. The main model function ties everything together to train the classifier and evaluate its performance.

Sigmoid function: The sigmoid function is used to map any real-valued number into the range (0, 1). This is essential in Logistic Regression as it helps in converting the linear combination of inputs and weights into a probability. It calculates the sigmoid activation using the formula: sigmoid(ùëß) = 1/(1+e^-z). We define our own sigmoid function and plot it using matplotlib.

Initializing Weights and Biases: We define a function that can initialize the weights and bias for our model. Using np.random.rand(dim, 1), we create a weight matrix of shape (dim, 1) with random values between 0 and 1, ensuring diverse starting points for model training. The bias is initialized to 0. The fixed seed (np.random.seed(0)) ensures reproducibility of the random values across different runs. 

Forward Propagation: We define the forward_prop function, which is integral to logistic regression. We use np.dot to compute the linear combination of weights and input features, add the bias term, and apply the sigmoid function to derive the activations. We then calculate the cross-entropy cost using np.sum and np.log, which measures the difference between predicted probabilities and true labels. This cost function, averaged across all samples, is crucial for gradient descent optimization, allowing us to adjust weights and biases to minimize prediction errors.

Backward Propagation: We define the back_prop function to calculate the gradients of the weights and bias. We start by determining the number of samples with X.shape[1]. We compute the gradient of the weights dw using np.dot to find the average gradient from the difference between predicted activations and actual labels. We then calculate the gradient of the bias db by averaging the sum of these differences. We store these gradients in a dictionary and return them. 

Propagate: We define the propagate function to handle both forward and backward propagation. First, we perform forward propagation by calling the forward_prop function, which computes the activations and cost. Next, we execute backward propagation using the back_prop function to calculate the gradients of the weights and bias. We then return both the gradients and the cost. 

Predict: We define the predict function to make predictions using the logistic regression model. First, we reshape the weight vector w to match the input feature dimensions. We compute the probability vector A by applying the sigmoid function to the dot product of w and the input features X, plus the bias b. We then iterate through the probabilities in A and set each element in Y_prediction to 1 if the probability exceeds 0.5, or 0 otherwise. Finally, we return Y_prediction, which contains the binary predictions for the given input data.

Get Accuracies: We define the get_accuracies function to calculate the accuracy of predictions. It starts by computing the absolute differences between the predicted labels and actual labels using np.abs(). It then calculates the mean of these differences with np.mean(), which represents the average error rate. The function computes the accuracy as 100 minus this mean difference multiplied by 100, converting the error rate into a percentage of correct predictions. Finally, it returns the accuracy value.

Optimize: We define the optimize function to train the logistic regression model. The function begins by initializing variables for tracking previous accuracies, storing costs, and setting the initial weights and bias. In each iteration, it calls the propagate function to obtain gradients and the cost, updates the weights and bias using the learning rate, and stores the cost. It then predicts labels for both the training and validation sets using the predict function. Accuracy for both sets is calculated using the get_accuracies function. If the validation accuracy improves and is not less than the training accuracy, it updates the best accuracy and stores the current weights, bias, and epoch. The function concludes by returning a dictionary of optimal values, including final weights, biases, accuracies, and predictions.

Model: We define the model function to train and evaluate the logistic regression model across different learning rates. Initially, the function sets up variables to track the best accuracies and initializes the weights and bias. For each learning rate in the specified list, the function reinitializes the weights and bias and then calls the optimize function to perform training. It compares the validation accuracy from the current learning rate with the best accuracy found so far, updating the best values if the new validation accuracy is higher. The function finally updates the dictionary with the best learning rate, weights, biases, and accuracy metrics. It returns this dictionary containing the optimal values and model performance details.

Training the Algorithm: We call the model function to train and evaluate the logistic regression model using the training and validation datasets. We specify 2000 iterations and a range of learning rates: [0.1, 0.0001, 0.001, 0.005]. The function iterates over these learning rates, optimizing the model for each one and selecting the best performing learning rate based on validation accuracy. The resulting best_values dictionary contains the optimal weights, biases, learning rate, and accuracy metrics, which are used to evaluate the model's performance and make predictions.

Evaluating the Algorithm: We use the predict function to generate predictions on the test set with the final weight and bias matrices stored in best_values. We calculate the model's accuracy on the test set using the get_accuracies function and find it to be 80.0%. We print this test accuracy along with the final model parameters. The output includes the best learning rate of 0.1, achieved at epoch 193, a training accuracy of 84.69%, and a validation accuracy of 84.0%. 

Visualizing Correctly and Incorrectly Classified Images: We use matplotlib to visualize the results of the classification model by plotting images from the test set. For correctly classified images, we display a cat image and a non-cat image by reshaping and plotting the corresponding entries from test_set_x and showing their predicted labels. For incorrectly classified images, we similarly plot a misclassified cat image and non-cat image, again using the reshaped test data and displaying their predicted labels. This allows us to visually assess the model‚Äôs performance and the accuracy of its predictions by comparing the images with their predicted and actual labels.
